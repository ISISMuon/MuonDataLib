{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1071fdfc-87cd-4f13-8b00-82ae9ffe8de7",
   "metadata": {},
   "source": [
    "# MuonDataLib Tutorial 5: One versus many filters\n",
    "\n",
    "In this tutorial we will investigate the filters in more detail. This includes:\n",
    "\n",
    "- The performance of one and numerous filters.\n",
    "- The reason for a small amount of filtering causing the histogram calculation to become slower.\n",
    "\n",
    "The first bit of code just sets up the problem. The frame start times will be used to create a series of filters that include different proportions of the full data set (e.g. 10%). This works best with large data sets, the current example is not really big enough to demonstrate the differences in the time taken for calculating the histograms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff31ebdd-ca26-4519-bc5f-18008de476b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from MuonDataLib.data.loader.load_events import load_events\n",
    "import time\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "file_name = 'HIFI00193325.nxs'\n",
    "input_file = os.path.join('..', 'Data_files', file_name)\n",
    "\n",
    "output_name = 'HIFI193325.nxs'\n",
    "output_file = os.path.join('..', 'Output_files', output_name)\n",
    "\n",
    "data = load_events(input_file, 64)\n",
    "frame_start_times = data.get_frame_start_times()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198bc758-1f70-42e7-ba76-155006b881bc",
   "metadata": {},
   "source": [
    "To get good statistics for the timing we will repeat the calculations `N_loops` times, which is currently set to 10. We will be filtering our data in increments of $10\\%$ (`percentages`). Finally the frame interval, `dt`, should be approximatly constant between two adjacent frames. A small amount is subtracted from `dt` to make sure that the filter that uses it will remain in a single frame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3bfc98-27e8-425d-8c8b-4dbd335c4bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_loops = 10\n",
    "percentages = np.asarray([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9])\n",
    "dt = frame_start_times[1] - frame_start_times[0] - 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e771c14-db36-4cee-8d41-89c91b32cd33",
   "metadata": {},
   "source": [
    "To keep the code clean a function will be used to calculate the timings for saving the histograms. The `clear_time_filters` is included to make sure that the `save_histogram` command does not use the cached values. Hence, the average will be for the same calculation every time. At the end of the function it returns the mean and standard deviation for the time taken to save the histogram nexus files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e52d96b-362d-48c4-aa42-8dec6e5555b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_stats(data):\n",
    "    times = np.zeros(N_loops)\n",
    "    for j in range(N_loops):\n",
    "        start = time.time()\n",
    "        data.save_histograms(output_file)\n",
    "        data.clear_time_filters()\n",
    "        times[j] = time.time() - start\n",
    "        os.remove(output_file)\n",
    "    return np.mean(times), np.std(times)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7751a9-a145-40ef-b19d-90a73f1c4936",
   "metadata": {},
   "source": [
    "We will now create some arrays to store the average time taken for the filtering. We will consider three cases:\n",
    "\n",
    "1. There is one large filter applied across all of the filtered data\n",
    "2. There are numerous filters of a single frame\n",
    "3. There are numerous overlapping filters, all of them will start at 0\n",
    "\n",
    "Then we will add the mean time for saving the whole data set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d04944-eb23-4ecf-80aa-6b990e2bca50",
   "metadata": {},
   "outputs": [],
   "source": [
    "times_many = np.zeros(len(percentages)+1)\n",
    "times_one = np.zeros(len(percentages)+1)\n",
    "times_overlap = np.zeros(len(percentages) + 1)\n",
    "\n",
    "mean, std = calc_stats(data)\n",
    "\n",
    "times_many[9] = mean\n",
    "times_one[9] = mean\n",
    "times_overlap[9] = mean\n",
    "print('full data', mean, std)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1646e2-c5be-4d2d-b246-651620155ebd",
   "metadata": {},
   "source": [
    "This code will loop over the percentages of the data to be filtered and calculates the mean time. The first block is a single filter, the second is lots of single frame filters and the last block is for the overlapping filters. Each time `clear_time_filters` is used to make sure that only the filters that we want to consider are included. The final bit of code prints and stores the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63f3824-8259-4f37-8975-d073351d996e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for j, percent in enumerate(percentages):\n",
    "    data.clear_time_filters()\n",
    "    data.add_time_filter('one', frame_start_times[0], frame_start_times[-1]*percent)\n",
    "    one_mean, one_std = calc_stats(data)\n",
    "    \n",
    "    data.clear_time_filters()\n",
    "    for k in range(int(len(frame_start_times)*percent)):\n",
    "        data.add_time_filter(f'filter {k}', frame_start_times[k], frame_start_times[k] + dt)\n",
    "    many_mean, many_std = calc_stats(data)\n",
    "    \n",
    "    data.clear_time_filters()\n",
    "    for k in range(int(len(frame_start_times)*percent)):\n",
    "        data.add_time_filter(f'filter {k}', frame_start_times[0], frame_start_times[k] + dt)\n",
    "    overlap_mean, overlap_std = calc_stats(data)\n",
    "    \n",
    "    pos = len(percentages) - j - 1\n",
    "    print(percent, many_mean, many_std, one_mean, one_std, overlap_mean, overlap_std)\n",
    "    times_many[pos] = many_mean\n",
    "    times_one[pos] = one_mean\n",
    "    times_overlap[pos] = overlap_mean\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd694ac-aef6-4d5b-ac11-5662139588d1",
   "metadata": {},
   "source": [
    "The plot tools expect a histogram, so we write the percentages as bin edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f275c9-9af4-4047-ada1-5de0f4431e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.asarray([0.05, 0.15, 0.25, 0.25, 0.45, 0.55, 0.65, 0.75, 0.85, 0.95, 1.05])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e182fde-0ea8-468d-b47a-a9614b3912ce",
   "metadata": {},
   "source": [
    "Finally we plot the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a5e98d-4f4f-46a0-b451-76f137be7d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from MuonDataLib.plot.basic import Figure\n",
    "\n",
    "fig = Figure(y_label='time to save (s)', x_label='fraction of data retained')\n",
    "fig.plot(bins, times_one, label='one filter')\n",
    "fig.plot(bins, times_many, label='many filters')\n",
    "fig.plot(bins, times_overlap, label='overlapping filters')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508c897f-0325-49fa-aea1-144aef472574",
   "metadata": {},
   "source": [
    "There are a few interesting features in this data. The first is that filtering a small amount of data is slower than using all of it. This seems unintuitive, as there is less data so it should be quicker. However, the code has to calculate which data to filter out (remove) before calculating the histogram. Hence, there is initially a time cost for doing the filtering if the only a small amount of data is removed. \n",
    "\n",
    "The second interesting thing is that using multiple filters makes the code slower. This is becuase each new filter causes a search of the frame times to identify which data to remove. Hence, as the number of searches increases the calculation gets slower.  \n",
    "\n",
    "The final interesting point, is that the `overlapping filters` and `many filters` methods take about the same amount of time for each fraction of data. This is because, th time taken scales with the number of filters and as a consequence the number of searches.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
